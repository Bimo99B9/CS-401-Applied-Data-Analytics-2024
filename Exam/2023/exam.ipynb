{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fc29c0d30afb83",
   "metadata": {},
   "source": [
    "# ADA final exam (Fall 2023)\n",
    "\n",
    "This exam consists of 2 parts. Parts are independent from each other.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "\n",
    "\"Friends\" is an American television sitcom that originally aired on NBC from September 22, 1994, to May 6, 2004. Created by David Crane and Marta Kauffman, the show gained immense popularity and has since become a classic in the world of television. The series is set in New York City and revolves around a group of six friends: Ross Geller (David Schwimmer), Rachel Green (Jennifer Aniston), Monica Geller (Courteney Cox), Chandler Bing (Matthew Perry), Joey Tribbiani (Matt LeBlanc), and Phoebe Buffay (Lisa Kudrow). The show explores their personal and professional lives as they navigate the ups and downs of relationships, careers, and the challenges of adulthood.\n",
    "\n",
    "In this exam, we will use a dataset containing all the conversations that occurred over 10 seasons of Friends. We refer to each row in the dataset as an 'utterance.\" The data format of the dataset is as follows\n",
    "\n",
    "- id: `<str>`, the index of the utterance in the format sAA_eBB_cCC_uDDD, where AA is the season number, BB is the episode number, CC is the scene/conversation number, and DDD is the number of the utterance in the scene (e.g. s01_e18_c05_u021).\n",
    "- speaker: `<str>`, the speaker who made the utterance, e.g. Monica Geller\n",
    "- conversation_id: `<str>`, the id of the first utterance in the conversation this utterance belongs. We assume conversations begin at the start of a new scene.\n",
    "- reply_to: `<str>`, the id of the utterance to which this utterance replies. None if the utterance is the first in a conversation.\n",
    "- text: `<str>`, the textual content of the utterance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b039cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import some required libraries!\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be188a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: The one where you find the protagonist (60 pts)\n",
    "\n",
    "A big debate among Friends fans is: who is the show's main character? In this task, your goal is to provide a data-driven answer to this question.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "**1.1 —** Load the data from the jsonl file `exam1.jsonl` into a pandas dataframe. Then\n",
    " \n",
    " A. Calculate and display the number of distinct speakers in the dataframe.\n",
    " \n",
    " B. Calculate and display the number of conversations (see `conversation_id`).\n",
    " \n",
    " C. Remove all utterances from the dataframe where the `speaker` is \"TRANSCRIPT_NOTE\" or \"#ALL#\". Print the number of rows in the dataframe.\n",
    " \n",
    " D. Create additional columns corresponding to the season (`season`, e.g., season 1 should contain `s01`) and the episode (`episode`, e.g., episode 5 of season 4 should contain `s04_e05`) of each utterance. Print the season and the episode associated with utterance `s10_e18_c11_u019`.\n",
    " \n",
    " E. Create an additional column corresponding to the length of each utterance in terms of the number of characters (`length`). Print the length associated with utterance `s10_e18_c11_u019`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4daa420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>reply-to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
       "      <td>Monica Geller</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s01_e01_c01_u002</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s01_e01_c01_u003</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>All right Joey, be nice. So does he have a hum...</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>s01_e01_c01_u002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s01_e01_c01_u004</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>s01_e01_c01_u003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s01_e01_c01_u005</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td></td>\n",
       "      <td>TRANSCRIPT_NOTE</td>\n",
       "      <td>s01_e01_c01_u004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s01_e01_c01_u006</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
       "      <td>Phoebe Buffay</td>\n",
       "      <td>s01_e01_c01_u005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>s01_e01_c01_u007</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>Okay, everybody relax. This is not even a date...</td>\n",
       "      <td>Monica Geller</td>\n",
       "      <td>s01_e01_c01_u006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>s01_e01_c01_u008</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>s01_e01_c01_u007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>s01_e01_c01_u009</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td></td>\n",
       "      <td>TRANSCRIPT_NOTE</td>\n",
       "      <td>s01_e01_c01_u008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s01_e01_c01_u010</td>\n",
       "      <td>s01_e01_c01_u001</td>\n",
       "      <td>Alright, so I'm back in high school, I'm stand...</td>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>s01_e01_c01_u009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id   conversation_id  \\\n",
       "0  s01_e01_c01_u001  s01_e01_c01_u001   \n",
       "1  s01_e01_c01_u002  s01_e01_c01_u001   \n",
       "2  s01_e01_c01_u003  s01_e01_c01_u001   \n",
       "3  s01_e01_c01_u004  s01_e01_c01_u001   \n",
       "4  s01_e01_c01_u005  s01_e01_c01_u001   \n",
       "5  s01_e01_c01_u006  s01_e01_c01_u001   \n",
       "6  s01_e01_c01_u007  s01_e01_c01_u001   \n",
       "7  s01_e01_c01_u008  s01_e01_c01_u001   \n",
       "8  s01_e01_c01_u009  s01_e01_c01_u001   \n",
       "9  s01_e01_c01_u010  s01_e01_c01_u001   \n",
       "\n",
       "                                                text          speaker  \\\n",
       "0  There's nothing to tell! He's just some guy I ...    Monica Geller   \n",
       "1  C'mon, you're going out with the guy! There's ...   Joey Tribbiani   \n",
       "2  All right Joey, be nice. So does he have a hum...    Chandler Bing   \n",
       "3                           Wait, does he eat chalk?    Phoebe Buffay   \n",
       "4                                                     TRANSCRIPT_NOTE   \n",
       "5  Just, 'cause, I don't want her to go through w...    Phoebe Buffay   \n",
       "6  Okay, everybody relax. This is not even a date...    Monica Geller   \n",
       "7                          Sounds like a date to me.    Chandler Bing   \n",
       "8                                                     TRANSCRIPT_NOTE   \n",
       "9  Alright, so I'm back in high school, I'm stand...    Chandler Bing   \n",
       "\n",
       "           reply-to  \n",
       "0              None  \n",
       "1  s01_e01_c01_u001  \n",
       "2  s01_e01_c01_u002  \n",
       "3  s01_e01_c01_u003  \n",
       "4  s01_e01_c01_u004  \n",
       "5  s01_e01_c01_u005  \n",
       "6  s01_e01_c01_u006  \n",
       "7  s01_e01_c01_u007  \n",
       "8  s01_e01_c01_u008  \n",
       "9  s01_e01_c01_u009  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('data/exam1.jsonl', lines=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f28259ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct speakers: 700\n",
      "Num conversations: 3107\n",
      "Distinct speakers: 698\n",
      "Num conversations: 3099\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distinct speakers: {len(df['speaker'].unique())}\")\n",
    "print(f\"Num conversations: {len(df['conversation_id'].unique())}\")\n",
    "\n",
    "df.drop(df.loc[df['speaker'] ==\"TRANSCRIPT_NOTE\"].index, inplace=True)\n",
    "df.drop(df.loc[df['speaker'] ==\"#ALL#\"].index, inplace=True)\n",
    "\n",
    "print(f\"Distinct speakers: {len(df['speaker'].unique())}\")\n",
    "print(f\"Num conversations: {len(df['conversation_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0180db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"season\"] = np.where(\n",
    "    df[\"id\"].str.contains(\"_\"), df[\"id\"].str.split(\"_\").str[0], df[\"id\"]\n",
    ")\n",
    "\n",
    "df[\"episode\"] = np.where(\n",
    "    df[\"id\"].str.contains(\"_\"), df[\"id\"].str.split(\"_\").str[1], df[\"id\"]\n",
    ")\n",
    "\n",
    "df['length']  = df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed290df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season: 67370    s10\n",
      "Name: season, dtype: object\n",
      "Episode: 67370    e18\n",
      "Name: episode, dtype: object\n",
      "utterance_length: 67370    17\n",
      "Name: utterance_length, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Season: {df[df['id']=='s10_e18_c11_u019']['season']}\")\n",
    "print(f\"Episode: {df[df['id']=='s10_e18_c11_u019']['episode']}\")\n",
    "print(f\"Length: {df[df['id']=='s10_e18_c11_u019']['length']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d3bdc7",
   "metadata": {},
   "source": [
    "**1.2** Next, you conduct some basic analyses:\n",
    "\n",
    " A. With `statsmodels`, fit a linear regression `length ~ C(season, Treatment(reference=\"s01\"))`, where length is an integer and season is a categorical variable. Print the regression summary.\n",
    " \n",
    " B. /**Discuss:/** Considering the regression summary:\n",
    "   - What does the intercept in this regression represent? \n",
    "   - What does the coefficient `C(season, Treatment(reference=\"s01\"))[T.s09]` represent? \n",
    "   - Does the average utterance in season 9 contain significantly more characters than in season 1 at the 0.05 significance level? Justify with the regression summary **only**. \n",
    "   - Does the average utterance in season 10 contain significantly more characters than in season 1 at the 0.05 significance level? Justify with the regression summary  **only** .\n",
    "\n",
    " C. Argue visually (i.e., with a plot) that there are 6 main characters in the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf0b3c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "PatsyError",
     "evalue": "Error evaluating factor: NameError: name 'length' is not defined\n    length ~ C(season, Treatment(reference=\"s01\"))\n    ^^^^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\compat.py:40\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[1;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\eval.py:179\u001b[0m, in \u001b[0;36mEvalEnvironment.eval\u001b[1;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[0;32m    178\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(expr, source_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVarLookupDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_namespaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'length' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43msmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength ~ C(season, Treatment(reference=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m))\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m res \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39msummary())\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\statsmodels\\base\\model.py:203\u001b[0m, in \u001b[0;36mModel.from_formula\u001b[1;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# with patsy it's drop or raise. let's raise.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 203\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_formula_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m ((endog, exog), missing_idx, design_info) \u001b[38;5;241m=\u001b[39m tmp\n\u001b[0;32m    206\u001b[0m max_endog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_formula_max_endog\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\statsmodels\\formula\\formulatools.py:63\u001b[0m, in \u001b[0;36mhandle_formula_data\u001b[1;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_util\u001b[38;5;241m.\u001b[39m_is_using_pandas(Y, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 63\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mdmatrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataframe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m         result \u001b[38;5;241m=\u001b[39m dmatrices(formula, Y, depth, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     67\u001b[0m                            NA_action\u001b[38;5;241m=\u001b[39mna_action)\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\highlevel.py:319\u001b[0m, in \u001b[0;36mdmatrices\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct two design matrices given a formula_like and data.\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03mThis function is identical to :func:`dmatrix`, except that it requires\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03mSee :func:`dmatrix` for details.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    318\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m EvalEnvironment\u001b[38;5;241m.\u001b[39mcapture(eval_env, reference\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 319\u001b[0m (lhs, rhs) \u001b[38;5;241m=\u001b[39m \u001b[43m_do_highlevel_design\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lhs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is missing required outcome variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\highlevel.py:164\u001b[0m, in \u001b[0;36m_do_highlevel_design\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_iter_maker\u001b[39m():\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([data])\n\u001b[1;32m--> 164\u001b[0m design_infos \u001b[38;5;241m=\u001b[39m \u001b[43m_try_incr_builders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m design_infos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m build_design_matrices(\n\u001b[0;32m    169\u001b[0m         design_infos, data, NA_action\u001b[38;5;241m=\u001b[39mNA_action, return_type\u001b[38;5;241m=\u001b[39mreturn_type\n\u001b[0;32m    170\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\highlevel.py:56\u001b[0m, in \u001b[0;36m_try_incr_builders\u001b[1;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(formula_like, ModelDesc):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_env, EvalEnvironment)\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdesign_matrix_builders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlhs_termlist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformula_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrhs_termlist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mNA_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\build.py:746\u001b[0m, in \u001b[0;36mdesign_matrix_builders\u001b[1;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m    743\u001b[0m factor_states \u001b[38;5;241m=\u001b[39m _factors_memorize(all_factors, data_iter_maker, eval_env)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# Now all the factors have working eval methods, so we can evaluate them\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# on some data to find out what type of data they return.\u001b[39;00m\n\u001b[1;32m--> 746\u001b[0m (num_column_counts, cat_levels_contrasts) \u001b[38;5;241m=\u001b[39m \u001b[43m_examine_factor_types\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_factors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_iter_maker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNA_action\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Now we need the factor infos, which encapsulate the knowledge of\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# how to turn any given factor into a chunk of data:\u001b[39;00m\n\u001b[0;32m    751\u001b[0m factor_infos \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\build.py:491\u001b[0m, in \u001b[0;36m_examine_factor_types\u001b[1;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_iter_maker():\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(examine_needed):\n\u001b[1;32m--> 491\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mfactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactor_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m cat_sniffers \u001b[38;5;129;01mor\u001b[39;00m guess_categorical(value):\n\u001b[0;32m    493\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m factor \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cat_sniffers:\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\eval.py:599\u001b[0m, in \u001b[0;36mEvalFactor.eval\u001b[1;34m(self, memorize_state, data)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, memorize_state, data):\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\eval.py:582\u001b[0m, in \u001b[0;36mEvalFactor._eval\u001b[1;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m, code, memorize_state, data):\n\u001b[0;32m    581\u001b[0m     inner_namespace \u001b[38;5;241m=\u001b[39m VarLookupDict([data, memorize_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_and_wrap_exc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mError evaluating factor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemorize_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_env\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danil\\Documents\\Github\\CS-401-Applied-Data-Analytics-2024\\venv39\\lib\\site-packages\\patsy\\compat.py:43\u001b[0m, in \u001b[0;36mcall_and_wrap_exc\u001b[1;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     42\u001b[0m     new_exc \u001b[38;5;241m=\u001b[39m PatsyError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (msg, e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e), origin)\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'length' is not defined\n    length ~ C(season, Treatment(reference=\"s01\"))\n    ^^^^^^"
     ]
    }
   ],
   "source": [
    "mod = smf.ols(formula='length ~ C(season, Treatment(reference=\"s01\"))', data=df)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb47cab",
   "metadata": {},
   "source": [
    "--- \n",
    "**1.3 —** Using `networkx` \n",
    "\n",
    "A. Create a `MultiDiGraph` (directed graph with self loops and parallel edges) where:\n",
    "- Each node $u$ is a character uniquely identified by the `speaker` field.\n",
    "- There is an edge between nodes $u$ and $v$ if $u$ replied to $v$. If an utterance (a row in the dataframe) is said in reply to nobody, then it will not correspond to an edge. Each edge should contain two attributes. Each edge should have two attributes: `season` and `episode`.\n",
    "\n",
    "B. Print the number of nodes and edges in your graph.\n",
    "\n",
    "C. **/Discuss:/** Instead of using multi-edges, what would be another way in which you could capture the number of replies associated with each node pair?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f362612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130a16f1",
   "metadata": {},
   "source": [
    "---\n",
    "With the graph ready, you set out to investigate who is the true protagonist of Friends.\n",
    "\n",
    "Ignore the graph you generated previously and instead use the graph provided in `exam2.graphml`. Note that this graph may be slightly different from what you generated, but treat it as the ground truth. We provide you with code to load the graph below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a917d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_graphml(\"./data/exam2.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad04b50",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee10e7",
   "metadata": {},
   "source": [
    "**1.4 —** Using the provided MultiDiGraph $G$:\n",
    "\n",
    "A. Calculate the out-degree of each node (also known as out-degree centrality). Please do not use the `nx.out_degree_centrality` function here, as it normalizes the degree. (E.g., if a node has 5 outgoing edges, it should have out-degree 5 according to your code.)\n",
    "\n",
    "B. Calculate the PageRank centrality of each node in $G$. Use the default parameters.\n",
    "\n",
    "C. Print both centrality metrics calculated above for the six main characters of Friends.\n",
    "\n",
    "D. **/Discuss:/** According to the metrics, who is the most important character in Friends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8407dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9f1601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1.5 —** **/True or false:/** Considering your answer in **1.4**, are the following statements true or false? Justify your answers. \n",
    "\n",
    "A. \"If we inverted all  edges in the graph such that an edge $(u,v)$ becomes an edge $(v,u)$, the PageRank centrality would remain unchanged.\"\n",
    "\n",
    "B. \"If we removed all outgoing edges from Rachel Green, her PageRank centrality would remain unchanged.\"\n",
    "\n",
    "C. \"If a new node was introduced in the graph, with 1,000 outgoing edges towards each other node, but no incoming edge, it would have the highest PageRank centrality.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf413c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e47355c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1.6 —** Next, you consider how these centrality metrics vary over the course of the seasons.\n",
    "\n",
    "A. Calculate the PageRank and out-degree centrality of the 6 main characters per episode, i.e., for each episode, create a graph containing only the utterances of that episode and calculate the PageRank centrality for this new graph. Print the PageRank and the out-degree of Rachel Green for the first episode of the first season.\n",
    "\n",
    "B. Considering the episode-level out-degree centrality of Phoebe Buffay in season 1 and in season 10, print the mean and the standard error of the mean.\n",
    "\n",
    "C. Create a single plot with 10 inches of width and 4 inches of height. The plot should contain two panels, containing the average PageRank centrality per season of Rachel Green and Ross Geller (Panel A), and the average out-degree per season of Rachel Green and Ross Geller (Panel B). Show 95% confidence intervals in your plot (calculated over the episodes in each season).\n",
    "\n",
    "D. **/Discuss:/** Does the plot support the hypothesis that Rachel was the show's protagonist in all 10 seasons? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09a998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94b0e5b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1.7 —** **/True or false:/** Considering your answer in **1.6** are the following statements true or false? Justify your answer. \n",
    "\n",
    "\n",
    "A. \"In season 7, Rachel Green's episode-level PageRank and out-degree centrality are higher than Ross Geller's. This difference is statistically significant at the 0.05 significance level.\"\n",
    "\n",
    "B. \"Phoebe Buffay's out-degree grew between season 1 and season 10; this implies that other characters spoke less than her in season 10.\"\n",
    "\n",
    "C. \"Phoebe Buffay's PageRank was higher in season 10 than in season 1. This difference is statistically significant at the 0.05 significance level and suggests that the character gained importance over the course of the show.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e48217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84866967",
   "metadata": {},
   "source": [
    "## Part 2: The one about text similarity (40 pts)\n",
    "\n",
    "Next, you investigate how unique characters are by analyzing what they said throughout the 10 seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a69b8",
   "metadata": {},
   "source": [
    "**2.1 —** Load the dataframe `exam3.jsonl`. This dataframe is similar to `exam1.jsonl`, except:\n",
    "\n",
    "- It has an additional column called `tokens`, containing a sentence list. Each sentence is another list composed of tokens, e.g.,\n",
    "`[['There', \"'s\", 'nothing', 'to', 'tell', '!'], ['He', \"'s\", 'just', 'some', 'guy', 'I', 'work', 'with', '!']]`.\n",
    "- It has an additional column called `episode` containing a unique episode identifier.\n",
    "- It only contains utterances by Phoebe, Rachel, Ross, Joey, Monica, or Chandler (the main characters).\n",
    " \n",
    "Given this dataframe, you will create an episode-level word-frequency matrix for Chandler Bing, one of the main characters.\n",
    "\n",
    "A. Create a list $L$ containing all distinct tokens uttered by Chandler Bing throughout the 10 seasons, sorted in ascending order. Print the 10 first and last elements of the list. \n",
    "\n",
    "B. Create a matrix $X$ with $m$ rows and $n$ columns, where: $n$ is the number of tokens in the list $L$ that you just created, and $m$ is the number of episodes (236). Each position $X_{i,j}$ in this matrix should contain the number of times the character uttered the word $j$ in episode $i$. Print how many times Chandler uttered the token `joey` in the first episode of the first season, as well as the shape of the matrix $X$.\n",
    "\n",
    "C. Transform the matrix $X$ into a TF-IDF matrix $T$, combining the following formula (as seen in class):\n",
    "\n",
    "$$\\text{TF}(i,j) = \\text{number of times the $j$-th word occurs in the $i$-th episode}$$\n",
    "\n",
    "$$\\text{IDF}(j) =  \\log \\frac{\\text{number of episodes}}{\\text{number of episodes in which the $j$-th word occurs}}$$\n",
    "\n",
    "Print the value in the TF-IDF matrix corresponding to Chandler's utterance of the token `joey` in the first episode of the first season.\n",
    "\n",
    "D. **/Discuss:/** Some of the tokens (e.g., `joey`) reference other characters. How may these tokens help a classifier predict which character uttered a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e951338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b691ae5",
   "metadata": {},
   "source": [
    "---\n",
    "For the remainder of the task, you will use the TF-IDF matrix that we compute below. Note:\n",
    "- This matrix was calculated in a slightly different way: it considers only the 1000 tokens with the highest term frequency.\n",
    "- We provide three useful variables below (`X`, `y`, and `df_tfidf`). \n",
    "    - `X` is a matrix containing the TF-IDF values for the top 1000 tokens, where each row corresponds to a character in an episode. \n",
    "    - `y` indicates which character is responsible for the utterance. Each character has a corresponding number, e.g., 2 for Monica Geller; see dictionary below. \n",
    "    - `df_tfidf` is a dataframe combining `X` with other episode and utterance-level metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0aebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (51312, 1000)\n",
      "y shape (51312,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>episode</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica Geller</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          speaker  episode    0    1    2    3    4    5    6    7  ...  990  \\\n",
       "0   Monica Geller  s01_e01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1  Joey Tribbiani  s01_e01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2   Chandler Bing  s01_e01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "   991  992  993  994  995  996  997  998  999  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[3 rows x 1002 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_tfidf = pd.read_json(\"./data/exam3.jsonl\", lines=True)[[\"speaker\", \"episode\", \"text\"]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_tfidf.text).toarray()\n",
    "print(\"X shape\", X.shape)\n",
    "\n",
    "\n",
    "map_char_to_int = {\n",
    "'Chandler Bing': 0,\n",
    "'Joey Tribbiani': 1,\n",
    "'Monica Geller': 2,\n",
    "'Phoebe Buffay': 3,\n",
    "'Rachel Green': 4,\n",
    "'Ross Geller': 5\n",
    "}\n",
    "\n",
    "\n",
    "y = df_tfidf.speaker.apply(lambda x: map_char_to_int[x]).values\n",
    "print(\"y shape\", y.shape)\n",
    "\n",
    "df_tfidf = pd.concat([df_tfidf[[\"speaker\", \"episode\"]],  pd.DataFrame(X)], axis=1)\n",
    "\n",
    "df_tfidf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f01045",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6b6f1",
   "metadata": {},
   "source": [
    "**2.2 —** To compare characters, carry out a classification task. Train a decision tree classifier to predict which main character uttered a sentence..\n",
    "\n",
    "A. Split the dataset into training and test sets using sklearn `sklearn.model_selection.train_test_split` using parameters `test_size=0.3` and `random_state=42`, and using the default values for all other parameters.\n",
    "\n",
    "B. Train a decision tree classifier (`sklearn.tree.DecisionTreeClassifier`) using `random_state=42`, leaving all other parameters as their default.\n",
    "\n",
    "C. Compute the accuracy of your classifier and of a random baseline, i.e., a classifier that predicts a character uniformly at random. **/Discuss:/** Compare the two accuracies.\n",
    "\n",
    "\n",
    "D. Compute the confusion matrix of your classifier using `sklearn.metrics.confusion_matrix`. Normalize the confusion matrix such that all cells sum to 1.\n",
    "\n",
    "E. Plot an appropriate graphical representation of the confusion matrix.\n",
    "\n",
    "F. **/Discuss:/** Analyzing the confusion matrix, discuss:\n",
    "   - Which character is most distinct in the way they talk?\n",
    "   - Which two characters are the most similar in the way they talk?\n",
    "   - Which two characters are the least similar in the way they talk?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8bf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a158a88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.3 —** **/Discuss:/** Last, you discuss the results obtained in **2.2** with a friend, who asks you some thought-provoking questions.\n",
    "\n",
    "A. Your friend proposes that you should create a measure of similarity between two characters in a given episode in a more direct way than what you've done in **2.2**.  Propose (but do not implement) said similarity metric.\n",
    "\n",
    "B. Your friend also suggests that your analysis might not truly capture how two characters differ. According to her, if people are in the same conversation, they might speak similarly simply because they are in the same social context. Propose (but do not implement) a way of creating a dataset where this confounder does not exist.\n",
    "\n",
    "C. Last, your friend complains about how you present your (normalized) confusion matrix. According to her, from reading the cells alone, it is unclear if the fraction of occurrences is higher or lower than what a random classifier would yield. Propose (but do not implement) a way of modifying the confusion matrix to address her concern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4253a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25c65d90",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
